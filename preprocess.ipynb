{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as p\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy as s\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from definitions import *\n",
    "from dataset_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process control switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_again = True\n",
    "# process_again = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IS_MASTER\n",
    "except: \n",
    "    IS_MASTER = False\n",
    "\n",
    "try:\n",
    "    ANALYZE\n",
    "except:\n",
    "    ANALYZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure spacy, create nltk stopwords set.\n",
    "\n",
    "***might be interesting to try POS features with and without negation words like no, not, n't***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = s.load('en_core_web_lg') # en_core_web_trf for accuracy\n",
    "stopwords = (set(nltk.corpus.stopwords.words('english')))\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code only needs to be run once at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_debates()\n",
    "# create_validation_subset()\n",
    "# sample_development_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_MASTER:\n",
    "    data = {}\n",
    "    \n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "\n",
    "    data_paths = {\n",
    "        'dev': [p.join(dev_path, 'dev.tsv'), p.join(dev_path, 'dev_spacy.pkl')],\n",
    "        'test': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'test', 'test_combined.tsv'),\n",
    "            p.join(PROC_DATA_DIR_PATH, 'test', 'test_spacy.pkl')\n",
    "        ],\n",
    "        'train': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'train', 'train_combined.tsv'),\n",
    "            p.join(PROC_DATA_DIR_PATH, 'train', 'train_spacy.pkl')\n",
    "        ],\n",
    "        # 'val': [\n",
    "        #     p.join(POLIT_DATA_DIR_PATH, 'val', 'val_combined.tsv'),\n",
    "        #     p.join(PROC_DATA_DIR_PATH, 'val', 'val_spacy.pkl')\n",
    "        # ],\n",
    "    }\n",
    "\n",
    "    if process_again:\n",
    "        for dtype, dpaths in data_paths.items():\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)\n",
    "    else:\n",
    "        for dtype, dpaths in data_paths.items():\n",
    "            if dtype == 'dev' and not p.exists(dpaths[0]):\n",
    "                sample_development_set()\n",
    "\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply spacy to content tokenized with nltk and joined. Extra step is done to utilise more sensitive nltk tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, df in data.items():\n",
    "    if 'spacy' not in df.columns:\n",
    "        df['spacy'] = [spacy(' '.join([t for t in word_tokenize(sent)])) for sent in df['content'].values]\n",
    "        df.to_pickle(data_paths[dtype][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize pickled selected spaCy features with thresholds:\n",
    "```json\n",
    "    {\n",
    "        'min_occ': {\n",
    "            'wstop': .01,\n",
    "            'wostop': .005,\n",
    "        },\n",
    "        'min_ratio_diff': {\n",
    "            'wstop': .8,\n",
    "            'wostop': .5,\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': {'wstop': array(['NUM', 'PROPN', 'NOUN', 'ADJ'], dtype=object), 'wostop': array(['NUM', 'SYM'], dtype=object)}, 'tag': {'wstop': array(['CD', 'NNP', 'VBG', 'NN', 'NNS', 'JJ'], dtype=object), 'wostop': array(['CD', '$'], dtype=object)}, 'dep': {'wstop': array(['pobj', 'compound', 'npadvmod', 'amod', 'nummod'], dtype=object), 'wostop': array(['pcomp', 'quantmod', 'nummod', 'nsubjpass'], dtype=object)}}\n"
     ]
    }
   ],
   "source": [
    "with open(p.join(PROC_DATA_DIR_PATH, 'selected_spacy_features.pkl'), 'rb') as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "print(selected_features)\n",
    "# pos_sel = selected_features['pos']\n",
    "# pos_sel = {v: i for i, v in enumerate(pos_sel)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for creating pos features. \n",
    "\n",
    "- `selection` is dict holding pos tags and their respective order in feature.\n",
    "- `is_one_hot` differentiate between one-hot encoding and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stylo_feat(feat_type, sent, selection, is_one_hot):\n",
    "    # feat type with underscore to match token attributes\n",
    "    ftwu = f'{feat_type}_'\n",
    "    feature = [0]*len(selection)\n",
    "\n",
    "    for t in sent:\n",
    "        t_attr = getattr(t, ftwu)\n",
    "        \n",
    "        if t_attr in selection:\n",
    "            if is_one_hot:\n",
    "                feature[selection[t_attr]] = 1\n",
    "            else:\n",
    "                feature[selection[t_attr]] += 1\n",
    "\n",
    "    # encode counts to binary and flatten\n",
    "    if not is_one_hot:\n",
    "        feature = [int(b) for digit in feature for b in f'{digit:06b}']\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare path for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = p.join(PROC_DATA_DIR_PATH, 'features')\n",
    "if not p.exists(features_path):\n",
    "    os.mkdir(features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature selection map used in creating stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_map = {\n",
    "    feat_type: {\n",
    "        stop_type: {\n",
    "            ft: ord for ord, ft in enumerate(selection)    \n",
    "        } for stop_type, selection in selected_features[feat_type].items()\n",
    "    } for feat_type in selected_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, df in data.items():\n",
    "    spacy_col = df['spacy'].values\n",
    "    features_df = df.loc[:, ['id']]\n",
    "\n",
    "    for feat_type in selected_features:\n",
    "        for stop_type in selected_features[feat_type]:\n",
    "            # one-hot\n",
    "            one_hot_df = pd.DataFrame([\n",
    "                create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], True)\n",
    "                for sent in spacy_col\n",
    "            ])\n",
    "            one_hot_col_base = f'{feat_type}_{stop_type}_one_hot'\n",
    "            one_hot_df.columns = [f'{one_hot_col_base}_{i}' for i in range(one_hot_df.shape[1])]\n",
    "\n",
    "            features_df = features_df.merge(one_hot_df, left_index=True, right_index=True)\n",
    "\n",
    "            counts_df = pd.DataFrame([\n",
    "                create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], False)\n",
    "                for sent in spacy_col\n",
    "            ])\n",
    "            counts_col_base = f'{feat_type}_{stop_type}_count'\n",
    "            counts_df.columns = [f'{counts_col_base}_{i}' for i in range(counts_df.shape[1])]\n",
    "\n",
    "            features_df = features_df.merge(counts_df, left_index=True, right_index=True)\n",
    "\n",
    "    # TODO: decide whether tsv or pickle is better\n",
    "    features_df.to_csv(\n",
    "        p.join(features_path, f'{dtype}_stylometric_features.tsv'),\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )\n",
    "    features_df.to_pickle(p.join(features_path, f'{dtype}_stylometric_features.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the length of the longest sentence for the LSTM padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I believe if we lead an air coalition, which we are now in the position of doing and intensify it, if we continue to build back up the Iraqi army, which has had some recent success in Ramadi, as you know, if we get back talking to the tribal sheiks in Anbar to try to rebuild those relationships, which were very successful, in going after Al Qaida in Iraq, if we get the Turks to pay more attention to ISIS than they're paying to the Kurds, if we do put together the kind of coalition with the specific tasks that I am outlining, I think we can be successful in destroying ISIS.\"]\n"
     ]
    }
   ],
   "source": [
    "ttc = data['train'].append(data['test'])\n",
    "ttc['length'] = ttc['content'].apply(lambda x: len(bert_tokenizer.tokenize(x)))\n",
    "# print(max(ttc['length'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ttc.loc[ttc['length'] == 135, 'content'].values)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea5e23d039880bc686ade8058e8989fedf6ebdbf20722602aab0e72521f2eedd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
