{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as p\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from definitions import *\n",
    "from model_helper_functions import *\n",
    "from dataset_helper_functions import *\n",
    "from bi_lstm import BiLSTM\n",
    "from bert_embedding_model import BertEmbeddingModel\n",
    "from debates_dataset import DebatesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IS_MASTER\n",
    "except: \n",
    "    IS_MASTER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_MASTER:\n",
    "    data, features = {}, {}\n",
    "    \n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "    features_path = p.join(PROC_DATA_DIR_PATH, 'features')\n",
    "\n",
    "    data_paths = {\n",
    "        'dev': [\n",
    "            p.join(dev_path, 'dev.tsv'),\n",
    "            # p.join(dev_path, 'dev_spacy.pkl'),\n",
    "            # p.join(features_path, 'dev_stylometric_features.pkl'),\n",
    "        ],\n",
    "        'test': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'test', 'test_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'test', 'test_spacy.pkl'),\n",
    "            # p.join(features_path, 'test_stylometric_features.pkl'),\n",
    "\n",
    "        ],\n",
    "        'train': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'train', 'train_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'train', 'train_spacy.pkl'),\n",
    "            # p.join(features_path, 'train_stylometric_features.pkl')\n",
    "        ],\n",
    "        'val': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'val', 'val_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'val', 'val_spacy.pkl'),\n",
    "            # p.join(features_path, 'val_stylometric_features.pkl'),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for dtype, dpaths in data_paths.items():\n",
    "        try:\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove validation records from train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df, test_df, train_df, val_df = data.values()\n",
    "rs, mp = 22, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "train_worthy = train_df.loc[train_df['label'] == 1]\n",
    "train_unworthy = train_df.loc[train_df['label'] == 0].sample(n=mp*len(train_worthy), random_state=rs, ignore_index=True)\n",
    "ratio = len(train_unworthy) / len(train_worthy)\n",
    "\n",
    "imbalanced_train = train_worthy.append(train_unworthy).sample(frac=1, random_state=rs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_worthy = val_df.loc[val_df['label'] == 1]\n",
    "val_unworthy = val_df.loc[val_df['label'] == 0].sample(n=mp*len(val_worthy), random_state=rs, ignore_index=True)\n",
    "\n",
    "balanced_val = val_worthy.append(val_unworthy).sample(frac=1, random_state=rs, ignore_index=True)\n",
    "\n",
    "test_worthy = test_df.loc[test_df['label'] == 1]\n",
    "test_unworthy = test_df.loc[test_df['label'] == 0].sample(n=mp*len(test_worthy), random_state=rs, ignore_index=True)\n",
    "\n",
    "balanced_test = test_worthy.append(test_unworthy).sample(frac=1, random_state=rs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap data in `DebatesDataset` class so that it can be passed to `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "transform = None\n",
    "\n",
    "dd_train = DebatesDataset(data=dev_df, transform=transform)\n",
    "dd_val = DebatesDataset(data=balanced_val, transform=transform)\n",
    "dd_test = DebatesDataset(data=balanced_test, transform=transform)\n",
    "\n",
    "# sent_level_feature_dim = len(dd_train[0][3]) if not transform else len(dd_train[0][4])\n",
    "\n",
    "loader_train = DataLoader(dd_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loader_val = DataLoader(dd_val, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loader_test = DataLoader(dd_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model, optimizer and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_model = BertEmbeddingModel(device=device)\n",
    "model = BiLSTM(\n",
    "    embedding_dim=embedding_model.dim,\n",
    "    sent_level_feature_dim=0\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss() #(pos_weight=torch.tensor([1.0]).to(device\n",
    "n_epochs = 50\n",
    "eval_period = len(loader_train) // 2\n",
    "best_val_loss = float(\"Inf\")\n",
    "exp_path = p.join(EXP_DIR_PATH, 'bi-lstm', 'test-bcewlogitsloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize running values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss, val_running_loss = 0.0, 0.0\n",
    "global_step = 0\n",
    "train_losses, val_losses, global_steps = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [21/2100], Train Loss: 0.6250, Validation Loss: 0.6005\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/model.pt\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/metrics.pt\n",
      "Epoch [1/50], Step [42/2100], Train Loss: 0.5574, Validation Loss: 0.5788\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/model.pt\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/metrics.pt\n",
      "Epoch [2/50], Step [63/2100], Train Loss: 0.4303, Validation Loss: 0.6314\n",
      "Epoch [2/50], Step [84/2100], Train Loss: 0.4427, Validation Loss: 0.5427\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/model.pt\n",
      "Model saved to ==> /home/jovyan/sharedstorage/s12b3v/dp/exp/bi-lstm/test-bcewlogitsloss/metrics.pt\n",
      "Epoch [3/50], Step [105/2100], Train Loss: 0.2794, Validation Loss: 0.6353\n",
      "Epoch [3/50], Step [126/2100], Train Loss: 0.3186, Validation Loss: 0.5971\n",
      "Epoch [4/50], Step [147/2100], Train Loss: 0.1968, Validation Loss: 0.6867\n",
      "Epoch [4/50], Step [168/2100], Train Loss: 0.1556, Validation Loss: 0.7195\n",
      "Epoch [5/50], Step [189/2100], Train Loss: 0.0710, Validation Loss: 0.9657\n",
      "Epoch [5/50], Step [210/2100], Train Loss: 0.0769, Validation Loss: 0.9688\n",
      "Epoch [6/50], Step [231/2100], Train Loss: 0.0292, Validation Loss: 1.0854\n",
      "Epoch [6/50], Step [252/2100], Train Loss: 0.0206, Validation Loss: 1.1544\n",
      "Epoch [7/50], Step [273/2100], Train Loss: 0.0199, Validation Loss: 1.0884\n",
      "Epoch [7/50], Step [294/2100], Train Loss: 0.0371, Validation Loss: 1.2390\n",
      "Epoch [8/50], Step [315/2100], Train Loss: 0.0399, Validation Loss: 0.9994\n",
      "Epoch [8/50], Step [336/2100], Train Loss: 0.0292, Validation Loss: 1.1791\n",
      "Epoch [9/50], Step [357/2100], Train Loss: 0.0199, Validation Loss: 1.1056\n",
      "Epoch [9/50], Step [378/2100], Train Loss: 0.0229, Validation Loss: 1.1808\n",
      "Epoch [10/50], Step [399/2100], Train Loss: 0.0056, Validation Loss: 1.4546\n",
      "Epoch [10/50], Step [420/2100], Train Loss: 0.0220, Validation Loss: 1.2262\n",
      "Epoch [11/50], Step [441/2100], Train Loss: 0.0070, Validation Loss: 1.2622\n",
      "Epoch [11/50], Step [462/2100], Train Loss: 0.0050, Validation Loss: 1.4669\n",
      "Epoch [12/50], Step [483/2100], Train Loss: 0.0017, Validation Loss: 1.4804\n",
      "Epoch [12/50], Step [504/2100], Train Loss: 0.0041, Validation Loss: 1.3891\n",
      "Epoch [13/50], Step [525/2100], Train Loss: 0.0009, Validation Loss: 1.4489\n",
      "Epoch [13/50], Step [546/2100], Train Loss: 0.0015, Validation Loss: 1.5146\n",
      "Epoch [14/50], Step [567/2100], Train Loss: 0.0008, Validation Loss: 1.5727\n",
      "Epoch [14/50], Step [588/2100], Train Loss: 0.0007, Validation Loss: 1.5130\n",
      "Epoch [15/50], Step [609/2100], Train Loss: 0.0007, Validation Loss: 1.5366\n",
      "Epoch [15/50], Step [630/2100], Train Loss: 0.0005, Validation Loss: 1.5609\n",
      "Epoch [16/50], Step [651/2100], Train Loss: 0.0004, Validation Loss: 1.5813\n",
      "Epoch [16/50], Step [672/2100], Train Loss: 0.0004, Validation Loss: 1.6654\n",
      "Epoch [17/50], Step [693/2100], Train Loss: 0.0004, Validation Loss: 1.6877\n",
      "Epoch [17/50], Step [714/2100], Train Loss: 0.0003, Validation Loss: 1.6670\n",
      "Epoch [18/50], Step [735/2100], Train Loss: 0.0003, Validation Loss: 1.6137\n",
      "Epoch [18/50], Step [756/2100], Train Loss: 0.0003, Validation Loss: 1.6763\n",
      "Epoch [19/50], Step [777/2100], Train Loss: 0.0003, Validation Loss: 1.6908\n",
      "Epoch [19/50], Step [798/2100], Train Loss: 0.0002, Validation Loss: 1.7020\n",
      "Epoch [20/50], Step [819/2100], Train Loss: 0.0002, Validation Loss: 1.6174\n",
      "Epoch [20/50], Step [840/2100], Train Loss: 0.0002, Validation Loss: 1.6695\n",
      "Epoch [21/50], Step [861/2100], Train Loss: 0.0002, Validation Loss: 1.7606\n",
      "Epoch [21/50], Step [882/2100], Train Loss: 0.0003, Validation Loss: 1.7058\n",
      "Epoch [22/50], Step [903/2100], Train Loss: 0.0002, Validation Loss: 1.8011\n",
      "Epoch [22/50], Step [924/2100], Train Loss: 0.0002, Validation Loss: 1.8245\n",
      "Epoch [23/50], Step [945/2100], Train Loss: 0.0002, Validation Loss: 1.7829\n",
      "Epoch [23/50], Step [966/2100], Train Loss: 0.0002, Validation Loss: 1.8750\n",
      "Epoch [24/50], Step [987/2100], Train Loss: 0.0002, Validation Loss: 1.7566\n",
      "Epoch [24/50], Step [1008/2100], Train Loss: 0.0001, Validation Loss: 1.8252\n",
      "Epoch [25/50], Step [1029/2100], Train Loss: 0.0001, Validation Loss: 1.8458\n",
      "Epoch [25/50], Step [1050/2100], Train Loss: 0.0001, Validation Loss: 1.7679\n",
      "Epoch [26/50], Step [1071/2100], Train Loss: 0.0001, Validation Loss: 1.8266\n",
      "Epoch [26/50], Step [1092/2100], Train Loss: 0.0001, Validation Loss: 1.8871\n",
      "Epoch [27/50], Step [1113/2100], Train Loss: 0.0001, Validation Loss: 1.9314\n",
      "Epoch [27/50], Step [1134/2100], Train Loss: 0.0001, Validation Loss: 1.9696\n",
      "Epoch [28/50], Step [1155/2100], Train Loss: 0.0001, Validation Loss: 1.7160\n",
      "Epoch [28/50], Step [1176/2100], Train Loss: 0.0001, Validation Loss: 1.8909\n",
      "Epoch [29/50], Step [1197/2100], Train Loss: 0.0001, Validation Loss: 1.8996\n",
      "Epoch [29/50], Step [1218/2100], Train Loss: 0.0001, Validation Loss: 1.8993\n",
      "Epoch [30/50], Step [1239/2100], Train Loss: 0.0001, Validation Loss: 1.9224\n",
      "Epoch [30/50], Step [1260/2100], Train Loss: 0.0001, Validation Loss: 1.9688\n",
      "Epoch [31/50], Step [1281/2100], Train Loss: 0.0001, Validation Loss: 2.0393\n",
      "Epoch [31/50], Step [1302/2100], Train Loss: 0.0001, Validation Loss: 1.7938\n",
      "Epoch [32/50], Step [1323/2100], Train Loss: 0.0001, Validation Loss: 1.9262\n",
      "Epoch [32/50], Step [1344/2100], Train Loss: 0.0001, Validation Loss: 1.9392\n",
      "Epoch [33/50], Step [1365/2100], Train Loss: 0.0001, Validation Loss: 1.9922\n",
      "Epoch [33/50], Step [1386/2100], Train Loss: 0.0001, Validation Loss: 1.9936\n",
      "Epoch [34/50], Step [1407/2100], Train Loss: 0.0001, Validation Loss: 1.9157\n",
      "Epoch [34/50], Step [1428/2100], Train Loss: 0.0001, Validation Loss: 2.0343\n",
      "Epoch [35/50], Step [1449/2100], Train Loss: 0.0001, Validation Loss: 1.9003\n",
      "Epoch [35/50], Step [1470/2100], Train Loss: 0.0001, Validation Loss: 2.0754\n",
      "Epoch [36/50], Step [1491/2100], Train Loss: 0.0001, Validation Loss: 1.9510\n",
      "Epoch [36/50], Step [1512/2100], Train Loss: 0.0001, Validation Loss: 1.9533\n",
      "Epoch [37/50], Step [1533/2100], Train Loss: 0.0001, Validation Loss: 2.0202\n",
      "Epoch [37/50], Step [1554/2100], Train Loss: 0.0001, Validation Loss: 2.0775\n",
      "Epoch [38/50], Step [1575/2100], Train Loss: 0.0001, Validation Loss: 2.0307\n",
      "Epoch [38/50], Step [1596/2100], Train Loss: 0.0001, Validation Loss: 1.8934\n",
      "Epoch [39/50], Step [1617/2100], Train Loss: 0.0001, Validation Loss: 1.9830\n",
      "Epoch [39/50], Step [1638/2100], Train Loss: 0.0000, Validation Loss: 2.1081\n",
      "Epoch [40/50], Step [1659/2100], Train Loss: 0.0001, Validation Loss: 2.0203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-db21d8b7c422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#         output = torch.sigmoid(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dp/dt/bi_lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, lengths, sent_level_features)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_level_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpacked_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 573\u001b[0;31m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for ids, sentences, labels, features in loader_train:\n",
    "        # print(sentences, labels)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        embeddings, lengths = embedding_model(sentences)\n",
    "        output = model(embeddings, lengths)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running values\n",
    "        running_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % eval_period == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_ids, val_sentences, val_labels, val_features in loader_val:\n",
    "                    val_labels = val_labels.float().to(device)\n",
    "                    val_embeddings, val_lengths = embedding_model(val_sentences)\n",
    "\n",
    "                    val_output = model(val_embeddings, val_lengths)\n",
    "                    # val_output = model(val_sentences)\n",
    "#                     val_output = torch.sigmoid(val_output)\n",
    "                    loss = criterion(val_output, val_labels)\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "            train_losses.append(running_loss / eval_period)\n",
    "            val_losses.append(val_running_loss / len(loader_val))\n",
    "            global_steps.append(global_step)\n",
    "\n",
    "            running_loss, val_running_loss = 0.0, 0.0\n",
    "\n",
    "\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{n_epochs}], '\n",
    "                f'Step [{global_step}/{n_epochs*len(loader_train)}], '\n",
    "                f'Train Loss: {train_losses[-1]:.4f}, '\n",
    "                f'Validation Loss: {val_losses[-1]:.4f}'\n",
    "            )\n",
    "\n",
    "            # TODO: early stopping here ?\n",
    "            if val_losses[-1] < best_val_loss:\n",
    "                best_val_loss = val_losses[-1]\n",
    "                save_checkpoint(\n",
    "                    p.join(exp_path, 'model.pt'),\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    val_loss=best_val_loss\n",
    "                )\n",
    "                save_metrics(\n",
    "                    p.join(exp_path, 'metrics.pt'),\n",
    "                    train_loss_list=train_losses,\n",
    "                    val_loss_list=val_losses,\n",
    "                    global_steps_list=global_steps   \n",
    "                )\n",
    "              \n",
    "save_metrics(\n",
    "    p.join(exp_path, 'metrics.pt'),\n",
    "    train_loss_list=train_losses,\n",
    "    val_loss_list=val_losses,\n",
    "    global_steps_list=global_steps\n",
    ")\n",
    "\n",
    "# save_params(\n",
    "#     p.join(exp_path, 'params.pt'),\n",
    "#     params={}\n",
    "# )\n",
    "\"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pred = []\n",
    "y_true = []\n",
    "temp = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_ids, test_sentences, test_labels, test_features in loader_test:           \n",
    "        test_labels = test_labels.float().to(device)\n",
    "\n",
    "        embeddings, lengths = embedding_model(test_sentences)\n",
    "        output = torch.sigmoid(model(embeddings, lengths))\n",
    "\n",
    "        temp.extend(output.tolist())\n",
    "        output = (output > threshold).int()\n",
    "        y_pred.extend(output.tolist())\n",
    "        y_true.extend(test_labels.tolist())\n",
    "\n",
    "print('Ranking:')\n",
    "temp.sort(reverse=True)\n",
    "print(temp[:10])\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n",
    "# print(f'precision: {tp/(tp+fp)}')\n",
    "# print(f'recall: {tp/(tp+fn)}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, digits=4)) # labels=[1,0], \n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['worthy', 'unworthy'])\n",
    "ax.yaxis.set_ticklabels(['worthy', 'unworthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0566b9ebc7a58507829be3d77002d1a3a0910233c54c7888c127aa3b7af58774"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
