{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as p\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy as s\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from definitions import *\n",
    "from dataset_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IS_MASTER\n",
    "except: \n",
    "    IS_MASTER = False\n",
    "\n",
    "try:\n",
    "    ANALYZE\n",
    "except:\n",
    "    ANALYZE = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure spacy, create nltk stopwords set.\n",
    "\n",
    "***might be interesting to try POS features with and without negation words like no, not, n't***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'did', \"mightn't\", 'such', \"shouldn't\", \"doesn't\", 'above', 'why', 'll', 'didn', 'whom', 'her', \"you'll\", 'them', 'then', 'so', 'needn', 'all', 'was', 'himself', 'd', 'can', 'he', 'and', 'there', 'she', 'when', 'who', \"shan't\", 'ourselves', 'of', 'most', 'while', \"should've\", 'how', 'once', 'haven', \"didn't\", 'any', \"mustn't\", 'only', \"haven't\", 'where', \"aren't\", 'just', 'each', 'through', 'don', 'ma', 'which', \"couldn't\", 'very', 'isn', 'their', 'have', 'mightn', 'no', 'o', \"she's\", 'from', 'before', 'me', 'those', 'my', 'at', 'm', 'shan', \"weren't\", \"needn't\", 'off', 'on', 'own', \"hasn't\", 't', 'should', 'do', 'ours', 'am', 'yours', 'our', \"you've\", 'few', 'him', 'being', 'over', 'other', 'not', 'y', 'more', 'theirs', 'same', 'these', 'to', 'hadn', 'your', 'hers', 'that', 'again', 'into', 's', 'will', 're', 'shouldn', 'against', 'were', 'its', 'by', \"isn't\", 'they', \"that'll\", 'doing', 'are', 'about', 'had', 'but', 'mustn', 'weren', 'yourselves', 'nor', 'itself', \"won't\", 'after', 'be', 'a', 'we', 'with', 'for', 'if', 'myself', 'aren', 'hasn', 'does', 'some', 'below', 'between', 'in', 'themselves', 'an', 'his', 'this', 'up', \"don't\", \"you're\", 'until', 'under', 'as', 'i', 'what', \"it's\", 'it', 'here', 'wasn', \"you'd\", 'too', 'during', 'having', 'herself', 'out', 'is', 'both', 'doesn', 'wouldn', 'the', 'been', 've', 'won', 'has', 'couldn', \"wasn't\", 'because', 'down', \"wouldn't\", 'you', 'or', 'ain', 'than', 'yourself', 'further', \"hadn't\", 'now'}\n"
     ]
    }
   ],
   "source": [
    "spacy = s.load('en_core_web_lg') # en_core_web_trf for accuracy\n",
    "stopwords = (set(nltk.corpus.stopwords.words('english')))\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code only needs to be run once at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_debates()\n",
    "# create_validation_subset()\n",
    "# sample_development_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_MASTER:\n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "    dev_tsv_path = p.join(dev_path, 'dev.tsv')\n",
    "    dev_pkl_path = p.join(dev_path, 'dev_spacy.pkl')\n",
    "\n",
    "    if p.exists(dev_pkl_path):\n",
    "        dev = pd.read_pickle(dev_pkl_path)\n",
    "    else:\n",
    "        if not p.exists(dev_tsv_path):\n",
    "            sample_development_set()\n",
    "\n",
    "        dev = pd.read_csv(dev_tsv_path, sep='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply spacy to content tokenized with nltk and joined. Extra step is done to utilise more sensitive nltk tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'spacy' not in dev.columns:\n",
    "    dev['spacy'] = dev['content'].apply(\n",
    "        lambda x: spacy(\n",
    "            ' '.join([t for t in word_tokenize(x)])\n",
    "        )\n",
    "    ).values\n",
    "    dev.to_pickle(p.join(dev_path, 'dev_spacy.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize pickled selected spaCy features with thresholds:\n",
    "```json\n",
    "    {\n",
    "        'min_occ': {\n",
    "            'wstop': .01,\n",
    "            'wostop': .005,\n",
    "        },\n",
    "        'min_ratio_diff': {\n",
    "            'wstop': .8,\n",
    "            'wostop': .5,\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': {'wstop': array(['NUM', 'PROPN', 'NOUN', 'ADJ'], dtype=object), 'wostop': array(['NUM', 'SYM'], dtype=object)}, 'tag': {'wstop': array(['CD', 'NNP', 'VBG', 'NN', 'NNS', 'JJ'], dtype=object), 'wostop': array(['CD', '$'], dtype=object)}, 'dep': {'wstop': array(['pobj', 'compound', 'npadvmod', 'amod', 'nummod'], dtype=object), 'wostop': array(['pcomp', 'quantmod', 'nummod', 'nsubjpass'], dtype=object)}}\n"
     ]
    }
   ],
   "source": [
    "with open(p.join(PROC_DATA_DIR_PATH, 'selected_spacy_features.pkl'), 'rb') as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "print(selected_features)\n",
    "# pos_sel = selected_features['pos']\n",
    "# pos_sel = {v: i for i, v in enumerate(pos_sel)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for creating pos features. \n",
    "\n",
    "- `selection` is dict holding pos tags and their respective order in feature.\n",
    "- `is_one_hot` differentiate between one-hot encoding and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stylo_feat(feat_type, sent, selection, is_one_hot):\n",
    "    # feat type with underscore to match token attributes\n",
    "    ftwu = f'{feat_type}_'\n",
    "    feature = [0]*len(selection)\n",
    "\n",
    "    for t in sent:\n",
    "        t_attr = getattr(t, ftwu)\n",
    "        \n",
    "        if t_attr in selection:\n",
    "            if is_one_hot:\n",
    "                feature[selection[t_attr]] = 1\n",
    "            else:\n",
    "                feature[selection[t_attr]] += 1\n",
    "\n",
    "    # encode counts to binary\n",
    "    if not is_one_hot:\n",
    "        feature = [f'{c:06b}' for c in feature]\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare path for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = p.join(PROC_DATA_DIR_PATH, 'features')\n",
    "if not p.exists(features_path):\n",
    "    os.mkdir(features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize features_df and prepare feature selection map used in creating stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_col = dev['spacy'].values\n",
    "features_df = dev.loc[:, ['id']]\n",
    "\n",
    "selected_features_map = {\n",
    "    feat_type: {\n",
    "        stop_type: {\n",
    "            ft: ord for ord, ft in enumerate(selection)    \n",
    "        } for stop_type, selection in selected_features[feat_type].items()\n",
    "    } for feat_type in selected_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat_type in selected_features:\n",
    "    for stop_type in selected_features[feat_type]:\n",
    "        # one-hot\n",
    "        features_df[f'{feat_type}_{stop_type}_one_hot'] = [\n",
    "            create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], True) for sent in spacy_col\n",
    "        ]\n",
    "        # counts\n",
    "        features_df[f'{feat_type}_{stop_type}_count'] = [\n",
    "            create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], False) for sent in spacy_col\n",
    "        ]\n",
    "\n",
    "# TODO: decide whether tsv or pickle is better\n",
    "features_df.to_csv(\n",
    "    p.join(features_path, 'stylometric_features.tsv'),\n",
    "    sep='\\t',\n",
    "    index=False\n",
    ")\n",
    "features_df.to_pickle(p.join(features_path, 'stylometric_features.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = dev.loc[0, 'content']\n",
    "# x = spacy(temp)\n",
    "# temp = dev[['id', 'content']]\n",
    "# temp['spacied'] = temp['content'].apply(lambda x: spacy(x))\n",
    "# # temp = ['tag_ | pos_ | dep_ | lemma_ | norm_']\n",
    "# # for t in x:\n",
    "# #     temp.append(f'{t.tag_} | {t.pos_} | {t.dep_} | {t.lemma_} | {t.norm_}')\n",
    "# print(temp.loc[0, 'spacied'][0].tag_)\n",
    "# [(i.text, i.pos_, i.) for i in x]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea5e23d039880bc686ade8058e8989fedf6ebdbf20722602aab0e72521f2eedd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
