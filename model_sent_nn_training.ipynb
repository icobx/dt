{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os.path as p\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import print_n_log\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from definitions import *\n",
    "from model_helper_functions import *\n",
    "from dataset_helper_functions import *\n",
    "from sent_nn import SentNN\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from debates_dataset import DebatesDataset\n",
    "from early_stopping import EarlyStopping\n",
    "from optuna.trial import TrialState\n",
    "from torchvision import transforms\n",
    "# my transforms\n",
    "from transforms import *\n",
    "from scorer.task5 import evaluate_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "optim_path = os.path.join(EXP_DIR_PATH, 'sent-nn', 'optimization')\n",
    "# bi-lstm\n",
    "# - no_feat\n",
    "# - sent_feat\n",
    "# - word_feat\n",
    "training_path = os.path.join(EXP_DIR_PATH, 'sent-nn', 'training')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_uw_ratio = 0\n",
    "slf_dim = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# studies = [m for m in os.listdir(optim_path) if m.split('_')[-1] != 'params.pkl']\n",
    "# models_directories = []\n",
    "# for m in studies:\n",
    "#     sp = m.split('_')[1:]\n",
    "#     xx = '_'.join([s for s in sp if s not in {'pNone', 'df0.2', 'wf0.2.pkl'}])\n",
    "#     if len(xx):\n",
    "#         models_directories.append(xx)\n",
    "\n",
    "# for c in models_directories:\n",
    "#     try:\n",
    "#         os.mkdir(os.path.join(training_path, c))\n",
    "#     except Exception as e:\n",
    "#         print(e.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "\n",
    "    data_paths = {\n",
    "        'dev': [\n",
    "            p.join(dev_path, 'dev.tsv'),\n",
    "        ],\n",
    "        'test': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'test', 'test_combined.tsv'),\n",
    "        ],\n",
    "        'train': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'train', 'train_combined.tsv'),\n",
    "        ],\n",
    "        'val': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'val', 'val_combined.tsv'),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for dtype, dpaths in data_paths.items():\n",
    "        try:\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "            exit()\n",
    "    \n",
    "    if training_on_weak[0]:\n",
    "        if training_on_weak[1] == 'balanced_original':\n",
    "            data['train'], _ = weak_data_merge(merge_type=training_on_weak[1])\n",
    "        else:\n",
    "            data['train'], data['val'] = weak_data_merge(merge_type=training_on_weak[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets and DataLoaders, takes trial as input to be able to suggest values for variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size, transforms_params=None, stopwords_type=None):\n",
    "    global train_uw_ratio, slf_dim\n",
    "\n",
    "    transform_pipeline = None\n",
    "\n",
    "    if transforms_params is not None:\n",
    "        transforms_map = {\n",
    "            'sum': Sum,\n",
    "            'onehot': OneHot,\n",
    "            'none': NoTransform\n",
    "        }\n",
    "        cw_map = {\n",
    "            'count_words': CountWords,\n",
    "            'none': NoTransform\n",
    "        }\n",
    "\n",
    "        from_sel = transforms_params['from_selection']\n",
    "  \n",
    "        pos_feat = transforms_map[transforms_params['pos_feature_type']]\n",
    "        pos_feat = pos_feat(\n",
    "            'pos', from_selection=from_sel, stopwords=stopwords_type\n",
    "        )\n",
    "\n",
    "        tag_feat = transforms_map[transforms_params['tag_feature_type']]\n",
    "        tag_feat = tag_feat(\n",
    "            'tag', from_selection=from_sel, stopwords=stopwords_type\n",
    "        )\n",
    "        \n",
    "        dep_feat = transforms_map[transforms_params['dep_feature_type']]\n",
    "        dep_feat = dep_feat(\n",
    "            'dep', from_selection=from_sel, stopwords=stopwords_type\n",
    "        )\n",
    "\n",
    "        cw_feat = cw_map[transforms_params['word_count_feature_type']]\n",
    "        cw_feat = cw_feat()\n",
    "        \n",
    "        transform_pipeline = transforms.Compose([\n",
    "            HandleStopwords(stopwords=stopwords_type),\n",
    "            pos_feat,\n",
    "            tag_feat,\n",
    "            dep_feat,\n",
    "            cw_feat,\n",
    "            ToBinary(6),\n",
    "            ToTensor()\n",
    "        ])\n",
    "    \n",
    "    print(transform_pipeline)\n",
    "    train = data['train']\n",
    "    worthy_train = train[train['label'] == 1]\n",
    "    train_uw_ratio = (len(train) - len(worthy_train)) / len(worthy_train)\n",
    "    print(train_uw_ratio)\n",
    "    \n",
    "    train_dd = DebatesDataset(data=data['train'], transform=transform_pipeline)\n",
    "    val_dd = DebatesDataset(data=data['val'], transform=transform_pipeline)\n",
    "    test_dd = DebatesDataset(data=data['test'], transform=transform_pipeline)\n",
    "    \n",
    "    if transform_pipeline:\n",
    "        slf_dim = train_dd[0][-1].size()[0]\n",
    "\n",
    "    train_loader = DataLoader(train_dd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dd, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup + training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(params, features_params=None, model_checkpoint_path=None):\n",
    "    global logf_path\n",
    "    # this is here so that it can be accessed here and in get_loaders()\n",
    "    stopwords_type = None\n",
    "    if features_params:\n",
    "        stopwords_type = features_params['stopwords_type'] if 'stopwords_type' in features_params else None\n",
    "\n",
    "    train_loader, val_loader, test_loader = get_loaders(\n",
    "        params['batch_size'],\n",
    "        transforms_params=features_params,\n",
    "        stopwords_type=stopwords_type\n",
    "    )\n",
    "\n",
    "    # best for given trial\n",
    "#     pooling_strategy = params['pooling_strategy']\n",
    "    dropout = params['dropout']\n",
    "#     hidden_dim = params['hidden_dim']\n",
    "#     n_hidden_layers = params['n_hidden_layers']\n",
    "    lr = params['learning_rate']\n",
    "    opt_weight_decay = params['optimizer_weigth_decay']\n",
    "    pos_weight = train_uw_ratio if params['pos_weight'] > 1.0 else 1.0\n",
    "    \n",
    "#     fnn_hidden_dim = params['fnn_hidden_dim']\n",
    "#     fnn_n_layers = params['fnn_n_hidden_layers']\n",
    "#     fnn_dropout = params['fnn_dropout']\n",
    "    \n",
    "#     pnn_hidden_dim = params['pnn_hidden_dim']\n",
    "#     pnn_dropout = params['pnn_dropout']\n",
    "       \n",
    "    emb_model_name = params['embedding_model_name']\n",
    "    emb_size_map = {\n",
    "        'all-mpnet-base-v2': 768,\n",
    "        'all-MiniLM-L6-v2': 384,\n",
    "        'multi-qa-mpnet-base-dot-v1': 768\n",
    "    }\n",
    "    # emb_model_name = 'all-MiniLM-L6-v2'\n",
    "    embedding_model = SentenceTransformer(emb_model_name, device=device, cache_folder=SBERT_MODEL_PATH)\n",
    "    \n",
    "    model = SentNN(\n",
    "        embeddings_dim=emb_size_map[emb_model_name],\n",
    "        sentence_level_feature_dim=slf_dim,\n",
    "        dropout=dropout,\n",
    "    ).to(device)    \n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=opt_weight_decay)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "#     criterion = nn.SmoothL1Loss(reduction='sum')\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=6,\n",
    "        path=model_checkpoint_path,\n",
    "        verbose=False,\n",
    "        trace_func=print_n_log.run('early_stopping', logf_path, 'DEBUG')\n",
    "    )\n",
    "\n",
    "    n_epochs = 30\n",
    "    threshold = 0.5\n",
    "    val_losses, train_losses, val_clf_reports, train_clf_reports = [], [], [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_val_losses, epoch_train_losses = [], []\n",
    "\n",
    "        model.train()\n",
    "        y_pred, y_true = [], []\n",
    "        for ids, sentences, labels, features in train_loader:\n",
    "            labels = labels.float().to(device)\n",
    "            features = features.to(device)\n",
    "            \n",
    "            embeddings = embedding_model.encode(sentences, convert_to_tensor=True, show_progress_bar=False).float()\n",
    "            output = model(embeddings, sent_level_features=features)\n",
    "            loss = criterion(output, labels)\n",
    "        \n",
    "            loss.backward()\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            pred = torch.sigmoid(output)\n",
    "            pred = (pred > threshold).int()\n",
    "            y_pred.extend(pred.tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        cr = classification_report(y_true, y_pred, digits=6, output_dict=True, zero_division=0)\n",
    "        train_clf_reports.append(cr)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        y_pred, y_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for val_ids, val_sentences, val_labels, val_features in val_loader:\n",
    "                val_labels = val_labels.float().to(device)\n",
    "                val_features = val_features.to(device)\n",
    "                \n",
    "                val_embeddings = embedding_model.encode(val_sentences, convert_to_tensor=True, show_progress_bar=False).float()\n",
    "                pred = model(val_embeddings, sent_level_features=val_features)\n",
    "                val_loss = criterion(pred, val_labels)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "                \n",
    "                pred = torch.sigmoid(pred)\n",
    "                \n",
    "                pred = (pred > threshold).int()\n",
    "                y_pred.extend(pred.tolist())\n",
    "                y_true.extend(val_labels.tolist())\n",
    "        \n",
    "        val_losses.append(np.average(epoch_val_losses))\n",
    "        train_losses.append(np.average(epoch_train_losses))\n",
    "        avg_val_loss = np.average(val_losses)\n",
    "        print(\n",
    "            'epoch ==> ', epoch,\n",
    "            ' | avg train loss ==> ', np.average(train_losses),\n",
    "            ' | avg val loss ==> ', avg_val_loss\n",
    "        )\n",
    "        print(classification_report(y_true, y_pred, digits=6, zero_division=0))\n",
    "        cr = classification_report(y_true, y_pred, digits=6, output_dict=True, zero_division=0)\n",
    "        val_clf_reports.append(cr)\n",
    "        \n",
    "        early_stopping(\n",
    "            val_loss=avg_val_loss,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_losses=train_losses,\n",
    "            val_losses=val_losses,\n",
    "            train_clf_reports=train_clf_reports,\n",
    "            val_clf_reports=val_clf_reports,\n",
    "            acomp_metrics=('f1_p', cr['1.0']['f1-score'])\n",
    "        )\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print('early stopping...')\n",
    "            break\n",
    "\n",
    "    # recall_p = early_stopping.acomp_metrics['recall_p'] if early_stopping.acomp_metrics else 0.0\n",
    "    \"Done.\"\n",
    "#     return model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_id(item: str):\n",
    "    item = str(int(item))\n",
    "    return int(item[:8]), int(item[8:])\n",
    "\n",
    "def evaluate_model(params, features_params=None, load_path=None, bam=''):\n",
    "    stopwords_type = None\n",
    "    if features_params:\n",
    "        stopwords_type = features_params['stopwords_type'] if 'stopwords_type' in features_params else None\n",
    "        \n",
    "    train_loader, val_loader, test_loader = get_loaders(\n",
    "        params['batch_size'],\n",
    "        transforms_params=features_params,\n",
    "        stopwords_type=stopwords_type\n",
    "    )\n",
    "\n",
    "    # best for given trial\n",
    "#     pooling_strategy = params['pooling_strategy']\n",
    "    dropout = params['dropout']\n",
    "#     hidden_dim = params['hidden_dim']\n",
    "#     n_hidden_layers = params['n_hidden_layers']\n",
    "    lr = params['learning_rate']\n",
    "    opt_weight_decay = params['optimizer_weigth_decay']\n",
    "    pos_weight = train_uw_ratio if params['pos_weight'] > 1.0 else 1.0\n",
    "    print('pos: ', pos_weight)\n",
    "    \n",
    "#     fnn_hidden_dim = params['fnn_hidden_dim']\n",
    "#     fnn_n_layers = params['fnn_n_hidden_layers']\n",
    "#     fnn_dropout = params['fnn_dropout']\n",
    "    \n",
    "#     pnn_hidden_dim = params['pnn_hidden_dim']\n",
    "#     pnn_dropout = params['pnn_dropout']\n",
    "    \n",
    "    emb_model_name = params['embedding_model_name']\n",
    "\n",
    "    \n",
    "    emb_size_map = {\n",
    "        'all-mpnet-base-v2': 768,\n",
    "        'all-MiniLM-L6-v2': 384,\n",
    "        'multi-qa-mpnet-base-dot-v1': 768\n",
    "    }\n",
    "    # emb_model_name = 'all-MiniLM-L6-v2'\n",
    "    embedding_model = SentenceTransformer(emb_model_name, device=device, cache_folder=SBERT_MODEL_PATH)\n",
    "    \n",
    "    model = SentNN(\n",
    "        embeddings_dim=emb_size_map[emb_model_name],\n",
    "        sentence_level_feature_dim=slf_dim,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=opt_weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]).to(device))\n",
    "#     criterion = nn.SmoothL1Loss(reduction='sum')\n",
    "    \n",
    "    load_checkpoint(load_path, model, optimizer, device, bam=bam)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_ids, test_sentences, test_labels, test_features in test_loader:           \n",
    "            test_labels = test_labels.float().to(device)\n",
    "            test_features = test_features.to(device)\n",
    "\n",
    "            embeddings = embedding_model.encode(test_sentences, convert_to_tensor=True, show_progress_bar=False).float()\n",
    "            output = torch.sigmoid(model(embeddings, sent_level_features=test_features))\n",
    "            \n",
    "            ids.extend(test_ids.tolist())\n",
    "            scores.extend(output.tolist())\n",
    "            output = (output > threshold).int()\n",
    "            y_pred.extend(output.tolist())\n",
    "            y_true.extend(test_labels.tolist())\n",
    "    \n",
    "#     df_scorer = pd.DataFrame(data=list(zip(ids, scores)), columns=['id', 'score'])\n",
    "#     df_date_index = df_scorer.apply(lambda row: split_id(row.id), axis=1, result_type='expand')\n",
    "#     df_scorer = pd.concat([df_date_index, df_scorer], axis=1) \\\n",
    "#         .rename(columns={0: 'date', 1: 'index'}) \\\n",
    "#         .sort_values(by=['date', 'index'], axis=0) \\\n",
    "#         .reset_index(drop=True)\n",
    "    predictions = list(zip(ids, scores))\n",
    "    predictions = sorted(\n",
    "        predictions, \n",
    "        key=lambda x: x[0]\n",
    "    )\n",
    "    _, _, avg_precision, rr, num_relevant = evaluate_v2(predictions)\n",
    "    print('Avg. precision: ', avg_precision)\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, digits=4)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Load pretrained SentenceTransformer: all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'dropout': 0.02, 'learning_rate': 0.00687658226159879, 'optimizer_weigth_decay': 0.00015098895105608427, 'pos_weight': 32.81159420289855, 'embedding_model_name': 'all-mpnet-base-v2'}\n",
      "None\n",
      "None\n",
      "1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9680d70d07c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7b847e1bc473>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params, features_params, model_checkpoint_path)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_level_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0msentences_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_sorted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# study_path = os.path.join(optim_path, 'bi-lstm_wAtt_sTPE_pNone_df0.2_wf0.2.pkl')\n",
    "# params_path = os.path.join(optim_path, 'bi-lstm_featOptim_wAtt_sTPE_pNone_df0.2_wf0.2_params.pkl')\n",
    "# model_checkpoint_path = os.path.join(training_path, 'wAtt_sTPE')\n",
    "# sent_nn_mF1_sTPE_pNone_df0.2_wf0.03\n",
    "# sent_nn_paramOptAfter_sentFeat_mF1_sTPE_pNone_df0.2_wf0.03\n",
    "model_optim_path = os.path.join(optim_path, 'sent_nn_NO_FEAT_V2_mF1_sTPE_pNone_df0.2_wf0.03')\n",
    "# feature_params_path = f'{model_optim_path}_featureParams.pkl'\n",
    "# params_path = f'{model_optim_path}_params.pkl'\n",
    "study_path = f'{model_optim_path}.pkl'\n",
    "\n",
    "\n",
    "# for now ignore features \n",
    "# studies = [s for s in studies if '_params' not in s and 'featOptim' not in s]\n",
    "is_training = True\n",
    "training_on_weak = (True, 'balanced_original')\n",
    "load_data()\n",
    "# bam = 'best_f1_p_'\n",
    "# bam = ''\n",
    "# for study_name in studies[:1]:\n",
    "# TODO: check whether all of these exist\n",
    "study = torch.load(os.path.join(optim_path, study_path))\n",
    "params = study.best_params\n",
    "print(params)\n",
    "features_params = None\n",
    "\n",
    "# features_params =  {'stopwords_type': 'wstop', 'from_selection': False, 'tag_feature_type': 'sum', 'word_count_feature_type': 'count_words'}\n",
    "# features_params = {}\n",
    "print(features_params)\n",
    "# todo: tu treba este w_feat!! 15:32 21.04.\n",
    "# del features_params['tag_feature_type']\n",
    "# del features_params['word_count_feature_type']\n",
    "# print(features_params)\n",
    "\n",
    "checkpoint_dir = 'no_feat_weak_balanced_original'\n",
    "# checkpoint_dir = [d for d in models_directories if d in study_name][0]\n",
    "study_log_name = 'sent-nn_noFeat_weakBalancedOriginal'\n",
    "# params = study.best_params\n",
    "logf_path = p.join(LOG_DIR_PATH, f'training_{study_log_name}.log')\n",
    "\n",
    "\n",
    "if features_params:\n",
    "    for ft in ['pos', 'tag', 'dep', 'word_count', 'word_level']:\n",
    "        feature_type = f'{ft}_feature_type'\n",
    "        if feature_type not in features_params:\n",
    "            features_params[feature_type] = 'none'\n",
    "\n",
    "if 'with_sequential_layer' not in params:\n",
    "    params['with_sequential_layer'] = False\n",
    "\n",
    "if isinstance(params['pos_weight'], bool):\n",
    "    params['pos_weight'] = 2.0 if params['pos_weight'] else 1.0\n",
    "\n",
    "checkpoint_path = os.path.join(training_path, checkpoint_dir)\n",
    "if is_training:\n",
    "    train_model(params, features_params, checkpoint_path)\n",
    "else:\n",
    "    evaluate_model(params, features_params, checkpoint_path, bam=bam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results\n",
    "- ***no_feat:***\n",
    "    - **last checkpoint:**\n",
    "    ```\n",
    "        Avg. precision:  0.13700088844162206\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9927    0.8118    0.8932      6328\n",
    "                 1.0     0.0760    0.7206    0.1375       136\n",
    "\n",
    "            accuracy                         0.8099      6464\n",
    "           macro avg     0.5343    0.7662    0.5154      6464\n",
    "        weighted avg     0.9734    0.8099    0.8773      6464\n",
    "    ```\n",
    "    - **best f1 checkpoint:**\n",
    "    ```\n",
    "        Avg. precision:  0.15217500236398074\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9930    0.8271    0.9025      6329\n",
    "                 1.0     0.0822    0.7259    0.1477       135\n",
    "\n",
    "            accuracy                         0.8250      6464\n",
    "           macro avg     0.5376    0.7765    0.5251      6464\n",
    "        weighted avg     0.9740    0.8250    0.8867      6464\n",
    "    ```\n",
    "    - **last checkpoint - weak_simple:**\n",
    "    ```\n",
    "        Avg. precision:  0.12957222811302888\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9939    0.6971    0.8194      6328\n",
    "                 1.0     0.0538    0.8015    0.1008       136\n",
    "\n",
    "            accuracy                         0.6993      6464\n",
    "           macro avg     0.5239    0.7493    0.4601      6464\n",
    "        weighted avg     0.9741    0.6993    0.8043      6464\n",
    "    ```\n",
    "    - **best f1 checkpoint - weak_simple:**\n",
    "    ```\n",
    "        Avg. precision:  0.12082746852589468\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9939    0.7159    0.8323      6329\n",
    "                 1.0     0.0562    0.7926    0.1049       135\n",
    "\n",
    "            accuracy                         0.7175      6464\n",
    "           macro avg     0.5250    0.7543    0.4686      6464\n",
    "        weighted avg     0.9743    0.7175    0.8171      6464\n",
    "    ```\n",
    "    - **last checkpoint - weak_balanced_result:**\n",
    "    ```\n",
    "        Avg. precision:  0.11848342811893009\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9944    0.7034    0.8240      6328\n",
    "                 1.0     0.0558    0.8162    0.1045       136\n",
    "\n",
    "            accuracy                         0.7058      6464\n",
    "           macro avg     0.5251    0.7598    0.4642      6464\n",
    "        weighted avg     0.9747    0.7058    0.8088      6464\n",
    "    ```\n",
    "    - **best f1 checkpoint - weak_balanced_result:**\n",
    "    ```\n",
    "        Avg. precision:  0.11507593855680112\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9938    0.7073    0.8264      6328\n",
    "                 1.0     0.0551    0.7941    0.1031       136\n",
    "\n",
    "            accuracy                         0.7092      6464\n",
    "           macro avg     0.5244    0.7507    0.4647      6464\n",
    "        weighted avg     0.9740    0.7092    0.8112      6464\n",
    "    ```\n",
    "    - **last checkpoint - weak_balanced_original:**:\n",
    "    ```\n",
    "        Avg. precision:  0.07622417503347415\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9913    0.7396    0.8471      6328\n",
    "                 1.0     0.0545    0.6985    0.1011       136\n",
    "\n",
    "            accuracy                         0.7387      6464\n",
    "           macro avg     0.5229    0.7190    0.4741      6464\n",
    "        weighted avg     0.9716    0.7387    0.8314      6464\n",
    "    ```\n",
    "    - **best f1 checkpoint - weak_balanced_original:**\\\n",
    "    ```\n",
    "        --\n",
    "    ```\n",
    "    \n",
    "- ***w_feat:***\n",
    "    - **last checkpoint:**\n",
    "    ```\n",
    "        Avg. precision:  0.12910844453372206\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9911    0.8110    0.8921      6328\n",
    "                 1.0     0.0700    0.6618    0.1266       136\n",
    "\n",
    "            accuracy                         0.8079      6464\n",
    "           macro avg     0.5306    0.7364    0.5093      6464\n",
    "        weighted avg     0.9717    0.8079    0.8760      6464\n",
    "    ```\n",
    "    \n",
    "    - **best f1 checkpoint:**\n",
    "    ```\n",
    "        Avg. precision:  0.12333415169590956\n",
    "        Classification Report:\n",
    "                      precision    recall  f1-score   support\n",
    "\n",
    "                 0.0     0.9912    0.8151    0.8946      6328\n",
    "                 1.0     0.0714    0.6618    0.1289       136\n",
    "\n",
    "            accuracy                         0.8119      6464\n",
    "           macro avg     0.5313    0.7384    0.5117      6464\n",
    "        weighted avg     0.9718    0.8119    0.8784      6464\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this is kept here just in case\n",
    "# optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "# needed for GridSampler\n",
    "# search_space = {\n",
    "#     'batch_size': [16, 32, 64],\n",
    "#     'pooling_strategy': ['last_four', 'last_four_sum', 'second_last'],\n",
    "# #     'should_scale_emb': [False, True],\n",
    "#     'dropout': [i/100 for i in range(0, 51, 5)],\n",
    "#     'hidden_dim': [128, 256, 512],\n",
    "#     'optimizer_weigth_decay': [i/10000 for i in range(11)],\n",
    "#     'learning_rate': round_to_first_non_zero([i/100000 for i in range_inc(0, 100000, 1, 10)]),\n",
    "#     'pos_weight': [1.0, train_uw_ratio]\n",
    "# }\n",
    "# feature_search_space = {\n",
    "#     'stopwords_type': ['wstop', 'wostop'],\n",
    "#     'from_selection': [True, False],\n",
    "#     'pos_feature_type': ['sum', 'onehot', 'none'],\n",
    "#     'tag_feature_type': ['sum', 'onehot', 'none'],\n",
    "#     'word_count_feature_type': ['count_words', 'none'],\n",
    "# #     'word_level_feature_type': ['dep', 'triplet']\n",
    "# }\n",
    "# # print(search_space)\n",
    "# params = {\n",
    "#     'batch_size': 32,\n",
    "#     'pooling_strategy': 'second_last',\n",
    "#     'dropout': 0.39,\n",
    "#     'hidden_dim': 256,\n",
    "#     'w_seq': True,\n",
    "#     'lr': 0.004118121,\n",
    "#     'opt_weight_decay': 0.024460049,\n",
    "#     'pos_weight': train_uw_ratio,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0566b9ebc7a58507829be3d77002d1a3a0910233c54c7888c127aa3b7af58774"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
