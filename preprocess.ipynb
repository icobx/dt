{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as p\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy as s\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from definitions import *\n",
    "from dataset_helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process control switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_again = True\n",
    "# process_again = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IS_MASTER\n",
    "except: \n",
    "    IS_MASTER = False\n",
    "\n",
    "try:\n",
    "    ANALYZE\n",
    "except:\n",
    "    ANALYZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure spacy, create nltk stopwords set.\n",
    "\n",
    "***might be interesting to try POS features with and without negation words like no, not, n't***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy = s.load('en_core_web_lg') # en_core_web_trf for accuracy\n",
    "stopwords = (set(nltk.corpus.stopwords.words('english')))\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code only needs to be run once at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_debates()\n",
    "# create_validation_subset()\n",
    "# sample_development_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_MASTER:\n",
    "    data = {}\n",
    "    \n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "\n",
    "    data_paths = {\n",
    "        'dev': [p.join(dev_path, 'dev.tsv'), p.join(dev_path, 'dev_spacy.pkl')],\n",
    "        'test': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'test', 'test_combined.tsv'),\n",
    "            p.join(PROC_DATA_DIR_PATH, 'test', 'test_spacy.pkl')\n",
    "        ],\n",
    "        'train': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'train', 'train_combined.tsv'),\n",
    "            p.join(PROC_DATA_DIR_PATH, 'train', 'train_spacy.pkl')\n",
    "        ],\n",
    "        # 'val': [\n",
    "        #     p.join(POLIT_DATA_DIR_PATH, 'val', 'val_combined.tsv'),\n",
    "        #     p.join(PROC_DATA_DIR_PATH, 'val', 'val_spacy.pkl')\n",
    "        # ],\n",
    "    }\n",
    "\n",
    "    if process_again:\n",
    "        for dtype, dpaths in data_paths.items():\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)\n",
    "    else:\n",
    "        for dtype, dpaths in data_paths.items():\n",
    "            if dtype == 'dev' and not p.exists(dpaths[0]):\n",
    "                sample_development_set()\n",
    "\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply spacy to content tokenized with nltk and joined. Extra step is done to utilise more sensitive nltk tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, df in data.items():\n",
    "    if 'spacy' not in df.columns:\n",
    "        df['spacy'] = [spacy(' '.join([t for t in word_tokenize(sent)])) for sent in df['content'].values]\n",
    "        df.to_pickle(data_paths[dtype][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deserialize pickled selected spaCy features with thresholds:\n",
    "```json\n",
    "    {\n",
    "        'min_occ': {\n",
    "            'wstop': .01,\n",
    "            'wostop': .005,\n",
    "        },\n",
    "        'min_ratio_diff': {\n",
    "            'wstop': .8,\n",
    "            'wostop': .5,\n",
    "        }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pos': {'wstop': array(['NUM', 'PROPN', 'NOUN', 'ADJ'], dtype=object), 'wostop': array(['NUM', 'SYM'], dtype=object)}, 'tag': {'wstop': array(['CD', 'NNP', 'VBG', 'NN', 'NNS', 'JJ'], dtype=object), 'wostop': array(['CD', '$'], dtype=object)}, 'dep': {'wstop': array(['pobj', 'compound', 'npadvmod', 'amod', 'nummod'], dtype=object), 'wostop': array(['pcomp', 'quantmod', 'nummod', 'nsubjpass'], dtype=object)}}\n"
     ]
    }
   ],
   "source": [
    "with open(p.join(PROC_DATA_DIR_PATH, 'selected_spacy_features.pkl'), 'rb') as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "print(selected_features)\n",
    "# pos_sel = selected_features['pos']\n",
    "# pos_sel = {v: i for i, v in enumerate(pos_sel)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method for creating pos features. \n",
    "\n",
    "- `selection` is dict holding pos tags and their respective order in feature.\n",
    "- `is_one_hot` differentiate between one-hot encoding and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stylo_feat(feat_type, sent, selection, is_one_hot):\n",
    "    # feat type with underscore to match token attributes\n",
    "    ftwu = f'{feat_type}_'\n",
    "    feature = [0]*len(selection)\n",
    "\n",
    "    for t in sent:\n",
    "        t_attr = getattr(t, ftwu)\n",
    "        \n",
    "        if t_attr in selection:\n",
    "            if is_one_hot:\n",
    "                feature[selection[t_attr]] = 1\n",
    "            else:\n",
    "                feature[selection[t_attr]] += 1\n",
    "\n",
    "    # encode counts to binary and flatten\n",
    "    if not is_one_hot:\n",
    "        feature = [int(b) for digit in feature for b in f'{digit:06b}']\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare path for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = p.join(PROC_DATA_DIR_PATH, 'features')\n",
    "if not p.exists(features_path):\n",
    "    os.mkdir(features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare feature selection map used in creating stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_map = {\n",
    "    feat_type: {\n",
    "        stop_type: {\n",
    "            ft: ord for ord, ft in enumerate(selection)    \n",
    "        } for stop_type, selection in selected_features[feat_type].items()\n",
    "    } for feat_type in selected_features\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save stylometric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dtype, df in data.items():\n",
    "    spacy_col = df['spacy'].values\n",
    "    features_df = df.loc[:, ['id']]\n",
    "\n",
    "    for feat_type in selected_features:\n",
    "        for stop_type in selected_features[feat_type]:\n",
    "            # one-hot\n",
    "            one_hot_df = pd.DataFrame([\n",
    "                create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], True)\n",
    "                for sent in spacy_col\n",
    "            ])\n",
    "            one_hot_col_base = f'{feat_type}_{stop_type}_one_hot'\n",
    "            one_hot_df.columns = [f'{one_hot_col_base}_{i}' for i in range(one_hot_df.shape[1])]\n",
    "\n",
    "            features_df = features_df.merge(one_hot_df, left_index=True, right_index=True)\n",
    "\n",
    "            counts_df = pd.DataFrame([\n",
    "                create_stylo_feat(feat_type, sent, selected_features_map[feat_type][stop_type], False)\n",
    "                for sent in spacy_col\n",
    "            ])\n",
    "            counts_col_base = f'{feat_type}_{stop_type}_count'\n",
    "            counts_df.columns = [f'{counts_col_base}_{i}' for i in range(counts_df.shape[1])]\n",
    "\n",
    "            features_df = features_df.merge(counts_df, left_index=True, right_index=True)\n",
    "\n",
    "    # TODO: decide whether tsv or pickle is better\n",
    "    features_df.to_csv(\n",
    "        p.join(features_path, f'{dtype}_stylometric_features.tsv'),\n",
    "        sep='\\t',\n",
    "        index=False\n",
    "    )\n",
    "    features_df.to_pickle(p.join(features_path, f'{dtype}_stylometric_features.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp = dev.loc[0, 'content']\n",
    "# x = spacy(temp)\n",
    "# temp = dev[['id', 'content']]\n",
    "# temp['spacied'] = temp['content'].apply(lambda x: spacy(x))\n",
    "# # temp = ['tag_ | pos_ | dep_ | lemma_ | norm_']\n",
    "# # for t in x:\n",
    "# #     temp.append(f'{t.tag_} | {t.pos_} | {t.dep_} | {t.lemma_} | {t.norm_}')\n",
    "# print(temp.loc[0, 'spacied'][0].tag_)\n",
    "# [(i.text, i.pos_, i.) for i in x]\n",
    "\n",
    "# x = [0, 9, 2, 1, 5, 67, 54]\n",
    "\n",
    "# feature = [int(b) for binary in x for b in f'{binary:06b}']\n",
    "# # len(x)\n",
    "# feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea5e23d039880bc686ade8058e8989fedf6ebdbf20722602aab0e72521f2eedd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
