{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8151)\n",
      "tensor(0.1801)\n"
     ]
    }
   ],
   "source": [
    "import os.path as p\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from definitions import *\n",
    "from model_helper_functions import *\n",
    "from dataset_helper_functions import *\n",
    "from bi_lstm import BiLSTM\n",
    "from bert_embedding_model import BertEmbeddingModel\n",
    "from DebatesDataset import DebatesDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IS_MASTER\n",
    "except: \n",
    "    IS_MASTER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_MASTER:\n",
    "    data, features = {}, {}\n",
    "    emb_model = 'all-mpnet-base-v2'#'multi-qa-mpnet-base-dot-v1'#'all-MiniLM-L6-v2'\n",
    "    \n",
    "    dev_path = p.join(PROC_DATA_DIR_PATH, 'dev')\n",
    "    features_path = p.join(PROC_DATA_DIR_PATH, 'features')\n",
    "\n",
    "    data_paths = {\n",
    "        'dev': [\n",
    "            p.join(dev_path, 'dev.tsv'),\n",
    "            # p.join(dev_path, 'dev_spacy.pkl'),\n",
    "            # p.join(features_path, 'dev_stylometric_features.pkl'),\n",
    "        ],\n",
    "        'test': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'test', 'test_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'test', 'test_spacy.pkl'),\n",
    "            # p.join(features_path, 'test_stylometric_features.pkl'),\n",
    "\n",
    "        ],\n",
    "        'train': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'train', 'train_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'train', 'train_spacy.pkl'),\n",
    "            # p.join(features_path, 'train_stylometric_features.pkl')\n",
    "        ],\n",
    "        'val': [\n",
    "            p.join(POLIT_DATA_DIR_PATH, 'val', 'val_combined.tsv'),\n",
    "            # p.join(PROC_DATA_DIR_PATH, 'val', 'val_spacy.pkl'),\n",
    "            # p.join(features_path, 'val_stylometric_features.pkl'),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for dtype, dpaths in data_paths.items():\n",
    "        try:\n",
    "            data[dtype] = pd.read_csv(dpaths[0], sep='\\t', index_col=False)\n",
    "        except Exception as e:\n",
    "            print(e.args)\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove validation records from train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df, test_df, train_df, val_df = data.values()\n",
    "rs = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "train_worthy = train_df.loc[train_df['label'] == 1]\n",
    "val_worthy = train_worthy.iloc[250:, :]\n",
    "train_worthy = train_worthy.iloc[:250, :]\n",
    "train_unworthy = train_df.loc[train_df['label'] == 0].iloc[:len(train_worthy)*2, :]\n",
    "ratio = len(train_unworthy) / len(train_worthy)\n",
    "\n",
    "imbalanced_train = train_worthy.append(train_unworthy).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_worthy = val_df.loc[val_df['label'] == 1]\n",
    "val_unworthy = val_df.loc[val_df['label'] == 0].sample(n=len(val_worthy), random_state=rs, ignore_index=True)\n",
    "\n",
    "balanced_val = val_worthy.append(val_unworthy).sample(frac=1, random_state=rs, ignore_index=True)\n",
    "\n",
    "test_worthy = test_df.loc[test_df['label'] == 1]\n",
    "test_unworthy = test_df.loc[test_df['label'] == 0].sample(n=len(test_worthy), random_state=rs, ignore_index=True)\n",
    "\n",
    "balanced_test = test_worthy.append(test_unworthy).sample(frac=1, random_state=rs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap data in `DebatesDataset` class so that it can be passed to `DataLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "transform = None\n",
    "\n",
    "dd_train = DebatesDataset(data=dev_df, transform=transform)\n",
    "dd_val = DebatesDataset(data=balanced_val, transform=transform)\n",
    "dd_test = DebatesDataset(data=balanced_test, transform=transform)\n",
    "\n",
    "sent_level_feature_dim = len(dd_train[0][3]) if not transform else len(dd_train[0][4])\n",
    "\n",
    "loader_train = DataLoader(dd_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loader_val = DataLoader(dd_val, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "loader_test = DataLoader(dd_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model, optimizer and criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "embedding_model = BertEmbeddingModel(device=device)\n",
    "model = BiLSTM(\n",
    "    embedding_dim=embedding_model.dim,\n",
    "    sent_level_feature_dim=sent_level_feature_dim\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "n_epochs = 5\n",
    "eval_period = len(loader_train) // 2\n",
    "best_val_loss = float(\"Inf\")\n",
    "exp_path = p.join(EXP_DIR_PATH, 'bi-lstm', 'test-bcewlogitsloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize running values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss, val_running_loss = 0.0, 0.0\n",
    "global_step = 0\n",
    "train_losses, val_losses, global_steps = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for ids, sentences, labels, features in loader_train:\n",
    "        labels = labels.float().to(device)\n",
    "        embeddings, lengths = embedding_model(sentences)\n",
    "\n",
    "        output, _ = model(embeddings, lengths)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running values\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % eval_period == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for val_ids, val_sentences, val_labels, val_features in loader_val:\n",
    "                    val_labels = val_labels.float().to(device)\n",
    "                    val_embeddings, val_lengths = embedding_model(sentences)\n",
    "\n",
    "                    output, _ = model(val_embeddings, val_lengths)\n",
    "\n",
    "                    loss = criterion(output, val_labels)\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "            train_losses.append(running_loss / eval_period)\n",
    "            val_losses.append(val_running_loss / len(loader_val))\n",
    "            global_steps.append(global_step)\n",
    "\n",
    "            running_loss, val_running_loss = 0.0, 0.0\n",
    "\n",
    "            print(\n",
    "                f'Epoch [{epoch+1}/{n_epochs}], '\n",
    "                f'Step [{global_step}/{n_epochs*len(loader_train)}], '\n",
    "                f'Train Loss: {train_losses[-1]:.4f}, '\n",
    "                f'Validation Loss: {val_losses[-1]:.4f}'\n",
    "            )\n",
    "\n",
    "            # TODO: early stopping here ?\n",
    "            if val_losses[-1] < best_val_loss:\n",
    "                best_val_loss = val_losses[-1]\n",
    "                save_checkpoint(\n",
    "                    p.join(exp_path, 'model.pt'),\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    val_loss=best_val_loss\n",
    "                )\n",
    "                save_metrics(\n",
    "                    p.join(exp_path, 'metrics.pt'),\n",
    "                    train_loss_list=train_losses,\n",
    "                    val_loss_list=val_losses,\n",
    "                    global_steps_list=global_steps   \n",
    "                )\n",
    "            model.train()\n",
    "              \n",
    "save_metrics(\n",
    "    p.join(exp_path, 'metrics.pt'),\n",
    "    train_loss_list=train_losses,\n",
    "    val_loss_list=val_losses,\n",
    "    global_steps_list=global_steps\n",
    ")\n",
    "\n",
    "# save_params(\n",
    "#     p.join(exp_path, 'params.pt'),\n",
    "#     params={}\n",
    "# )\n",
    "\"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pred = []\n",
    "y_true = []\n",
    "temp = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_ids, test_sentences, test_labels in loader_test:           \n",
    "        test_labels = test_labels.float().to(device)\n",
    "        output = model(test_sentences)\n",
    "\n",
    "        temp.extend(output.tolist())\n",
    "        output = (output > threshold).int()\n",
    "        y_pred.extend(output.tolist())\n",
    "        y_true.extend(test_labels.tolist())\n",
    "\n",
    "print('Ranking:')\n",
    "temp.sort(reverse=True)\n",
    "print(temp[:10])\n",
    "\n",
    "print(f'Accuracy: {accuracy_score(y_true, y_pred)}')\n",
    "# print(f'precision: {tp/(tp+fp)}')\n",
    "# print(f'recall: {tp/(tp+fn)}')\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, digits=4)) # labels=[1,0], \n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, cmap='Blues', fmt=\"d\")\n",
    "\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['worthy', 'unworthy'])\n",
    "ax.yaxis.set_ticklabels(['worthy', 'unworthy'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0566b9ebc7a58507829be3d77002d1a3a0910233c54c7888c127aa3b7af58774"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_dp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
